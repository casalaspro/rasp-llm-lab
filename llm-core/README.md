# LLM Core Service

A well-structured FastAPI-based service for text generation using Language Models.

## ğŸ—ï¸ Project Structure

```
llm-core/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py          # Package initialization
â”‚   â”œâ”€â”€ main.py              # FastAPI application entry point
â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”œâ”€â”€ models.py            # Pydantic models for API
â”‚   â””â”€â”€ routes.py            # API route handlers
â”œâ”€â”€ .env                     # Environment variables (local)
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ Dockerfile              # Container configuration
â”œâ”€â”€ run_local.py            # Python startup script
â””â”€â”€ start-local.ps1         # PowerShell startup script
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- pip package manager

### Local Development

#### Option 1: PowerShell Script (Recommended for Windows)
```powershell
.\start-local.ps1
```

#### Option 2: Python Script
```bash
python run_local.py
```

#### Option 3: Manual Commands
```bash
# Install dependencies
pip install -r requirements.txt

# Start the server
python -m uvicorn app.main:app --host 127.0.0.1 --port 8001 --reload
```

## ğŸ“¡ API Endpoints

Once the server is running, you can access:

- **Server**: `http://127.0.0.1:8001`
- **Interactive API Docs**: `http://127.0.0.1:8001/docs`
- **Alternative API Docs**: `http://127.0.0.1:8001/redoc`

### Available Endpoints

#### Health Check
- **GET** `/` - Basic service information
- **GET** `/health` - Detailed health check

#### Text Generation
- **POST** `/generate` - Generate text from a prompt

#### Example Usage

```bash
# Health check
curl http://127.0.0.1:8001/

# Generate text
curl -X POST http://127.0.0.1:8001/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Hello, how are you?",
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

## âš™ï¸ Configuration

The application uses environment variables for configuration. Copy and modify the `.env` file:

```bash
# Application settings
APP_NAME=llm-core
APP_VERSION=1.0.0
ENVIRONMENT=development

# Server settings
HOST=0.0.0.0
PORT=8001

# Debug settings
DEBUG=true
LOG_LEVEL=info

# Future LLM settings
# LLM_MODEL=your-model-name
# LLM_API_KEY=your-api-key
# LLM_MAX_TOKENS=1000
# LLM_TEMPERATURE=0.7
```

## ğŸ³ Docker

Build and run with Docker:

```bash
# Build the image
docker build -t llm-core .

# Run the container
docker run -p 8001:8001 llm-core
```

## ğŸ§ª Testing

The service currently returns mock responses for testing pipeline integration between services. This will be replaced with actual LLM integration in the future.

## ğŸ—ï¸ Architecture

- **config.py**: Centralized configuration using Pydantic Settings
- **models.py**: API request/response models with validation
- **routes.py**: Clean separation of route handlers
- **main.py**: FastAPI application setup and configuration

## ğŸ”œ Future Enhancements

- Real LLM integration (OpenAI, Anthropic, local models)
- Authentication and rate limiting
- Request/response caching
- Metrics and monitoring
- Async request processing